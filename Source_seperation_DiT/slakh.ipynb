{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "from ipywidgets import Audio, widgets\n",
    "from IPython.display import display\n",
    "\n",
    "SAMPLE_RATE = 22050  # Define the sample rate for your audio\n",
    "\n",
    "def to_audio_widget(wav: torch.Tensor, normalize: bool = False):\n",
    "    assert len(wav.shape) == 2, f\"Expected 2D tensor, got shape: {wav.shape}\"\n",
    "    \n",
    "    # Convert tensor to numpy array and sum across channels to make mono if needed\n",
    "    audio_data = wav.sum(dim=0, keepdims=True).cpu().numpy()\n",
    "    \n",
    "    # Create the audio widget using keyword arguments\n",
    "    return Audio(value=audio_data, rate=SAMPLE_RATE, normalize=normalize)\n",
    "\n",
    "def wrap_in_out(*obj):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "        display(*obj)\n",
    "    return out\n",
    "\n",
    "def grid_widget(grid_of_objs):\n",
    "    col_boxes = []\n",
    "    for row_of_objs in grid_of_objs:\n",
    "        row_outs = []\n",
    "        for obj in row_of_objs:\n",
    "            row_outs += [obj]\n",
    "        col_boxes += [widgets.HBox(row_outs)]\n",
    "    return widgets.VBox(col_boxes)\n",
    "\n",
    "def save_audio(wav: torch.Tensor, filename: str):\n",
    "    # Convert tensor to numpy array\n",
    "    audio_data = wav.sum(dim=0, keepdims=True).cpu().numpy()\n",
    "    \n",
    "    # Normalize the audio if needed (to the range of [-1, 1])\n",
    "    if audio_data.max() > 1 or audio_data.min() < -1:\n",
    "        audio_data = audio_data / np.max(np.abs(audio_data))\n",
    "    \n",
    "    # Save the numpy array as a .wav file using scipy\n",
    "    write(filename, SAMPLE_RATE, audio_data.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 262144])\n"
     ]
    }
   ],
   "source": [
    "#读取sample.png文件为一个tensor\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the image\n",
    "image_path = 'sample.png'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Define the transformation to convert the image to a tensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Apply the transformation to the image\n",
    "image_tensor = transform(image)\n",
    "# Check the shape of the tensor\n",
    "audio_tensor = torch.randn(3,512*512)\n",
    "print(audio_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 152 tracks.\n",
      "* skipped because sources are not aligned!\n",
      "[5710592. 5710592. 5710592. 5529856. 5529856. 5529856. 6424320. 6424320.\n",
      " 6424320. 5435392. 5435392. 8514048. 8514048. 8514048. 6312448. 6312448.\n",
      " 4978688. 4978688. 4978688. 7500032. 7500032. 7500032. 5928960. 5928192.\n",
      " 5928960. 9242368. 9242368. 9242368. 6868992. 6868992. 6868992. 6195968.\n",
      " 6195968. 6195968. 5946880. 5946880. 5946880. 3969280. 3969280. 3969280.\n",
      " 4957184. 4957184. 4957184. 2765312. 2765312. 2765312. 6243584. 6243584.\n",
      " 6243584. 6684160. 6684160. 6684160. 6381056. 6381056. 6381056. 3714816.\n",
      " 3714816. 3714816. 5682944. 5682944. 5682944. 5526016. 5526016. 5526016.\n",
      " 6045440. 6045440. 6045440. 5372928. 5372928. 5372928. 6481664. 6481664.\n",
      " 6481664. 5613312. 5613312. 5613312. 5755136. 5755136. 5755136. 5439488.\n",
      " 5439488. 5439488. 3741184. 3741184. 3741184. 3793920. 3793920. 3793920.\n",
      " 7833344. 7833344. 7833344. 5648896. 5648896. 5648896. 6258944. 6258944.\n",
      " 6258944. 4614912. 4614912. 4614912. 5891840. 5891840. 5891840. 3677440.\n",
      " 3677440. 3677440. 4531712. 4531712. 4531712. 4605440. 4605440. 4605440.\n",
      " 5400576. 5400576. 5400576. 7364352. 7364352. 7364352. 5124864. 5124864.\n",
      " 5124864. 5509120. 5509120. 5509120. 5555712. 5555712. 5555712. 4543488.\n",
      " 4543488. 4543488. 6004480. 6004480. 6004480. 4440064. 4440064. 4440064.\n",
      " 4735488. 4735488. 4735488. 6049024. 6049024. 6049024. 4484608. 4484608.\n",
      " 4484608. 8467456. 8467456. 8467456. 5668864. 5668864. 5668864. 5464576.\n",
      " 5464576. 5233664. 5233664. 5233664. 4761600. 4761600. 4761600. 4605440.\n",
      " 4605440. 4605440. 5928960. 5928960. 5928960. 6211840. 6211840. 6211840.\n",
      " 4831232. 4831232. 4831232. 9885184. 9885184. 9885184. 5856512. 5856512.\n",
      " 5856512. 5592832. 5592832. 5592832. 7670272. 7670272. 7670272. 5658112.\n",
      " 5658112. 5658112. 5225472. 5225472. 5225472. 5080064. 5080064. 5080064.\n",
      " 5560832. 5560832. 5560832. 6217472. 6217472. 6217472. 7182080. 7182080.\n",
      " 7182080. 5896192. 5896192. 5896192. 4993024. 4993024. 4993024. 5808896.\n",
      " 5808896. 5808896. 5795072. 5795072. 5795072. 5161728. 5161728. 5161728.\n",
      " 6685184. 6685184. 6685184. 8978176. 8978176. 8978176. 5928960. 5928960.\n",
      " 5928960. 3681792. 3681792. 3681792. 5816064. 5816064. 5816064. 6203136.\n",
      " 6203136. 6203136. 6417664. 6417664. 6417664. 6675968. 6675968. 6675968.\n",
      " 9463808. 9463808. 9463808. 7500032. 7500032. 7500032. 4593152. 4593152.\n",
      " 4593152. 5433344. 5433344. 5433344. 6507776. 6507776. 6507776. 4710400.\n",
      " 4710400. 4710400. 5212928. 5212928. 5212928. 4265216. 4265216. 4265216.\n",
      " 8599296. 8599296. 8599296. 6198528. 6198528. 6198528. 6444800. 6444800.\n",
      " 6444800. 4735488. 4735488. 4735488. 4556032. 4556032. 4556032. 6196224.\n",
      " 6196224. 6196224. 4058112. 4058112. 4058112. 8386304. 8386304. 8386304.\n",
      " 9603840. 9603840. 9603840. 5799168. 5799168. 5799168. 5584384. 5584384.\n",
      " 4784640. 4784640. 4784640. 6246400. 6246400. 6246400. 6515968. 6515968.\n",
      " 6244352. 6244352. 6244352. 5640448. 5640448. 5640448. 5432064. 5432064.\n",
      " 5432064. 3661824. 3661824. 3661824. 5928960. 5928960. 5928960. 7833344.\n",
      " 7833344. 7833344. 6637056. 6637056. 6637056. 5913600. 5913600. 5913600.\n",
      " 5376768. 5376768. 5376768. 7208192. 7208192. 7208192. 6531584. 6531584.\n",
      " 6531584. 5221120. 5221120. 5221120. 5377280. 5377280. 5377280. 4332032.\n",
      " 4332032. 4332032. 5981184. 5981184. 5537536. 5537536. 5537536. 8099584.\n",
      " 8099584. 8099584. 6537728. 6537728. 6537728. 4319744. 4319744. 4319744.\n",
      " 5314560. 5314560. 5314560. 5525504. 5525504. 5525504. 5064192. 5064192.\n",
      " 5064192. 6526976. 6526976. 7244032. 7244032. 7244032. 5312256. 5312256.\n",
      " 5312256. 7032064. 7032064. 7032064. 4740864. 4740864. 4740864. 8055296.\n",
      " 8055296. 8055296. 5484032. 5484032. 5484032. 4822016. 4822016. 4822016.\n",
      " 5528064. 5528064. 5528064. 5723392. 5723392. 5723392. 6604544. 6604544.\n",
      " 6604544. 3913472. 3913472. 3913472. 6081280. 6081280. 6081280. 4059392.\n",
      " 4059392. 4059392. 3547136. 3547136. 3547136. 6475776. 6475776. 6474240.\n",
      " 6518784. 6518784. 6518784. 5266176. 5266176. 5048320. 5048320. 5048320.\n",
      " 4831232. 4831232. 4831232. 4692224. 4692224. 4692224. 5135872. 5135872.\n",
      " 5135872. 6350848. 6350848. 6350848. 6082560. 6082560. 6082560. 3890432.\n",
      " 3890432. 3890432. 1733376. 1733376. 1733376.]\n",
      "Track02084 skipped because sources are not aligned!\n",
      "[6475776. 6475776. 6474240.]\n",
      "Track01886 skipped because sources are not aligned!\n",
      "[5928960. 5928192. 5928960.]\n",
      "self.sr=22050, min: 12, max: 640\n",
      "Keeping 149 of 152 tracks\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data import MultiSourceDataset\n",
    "testset = MultiSourceDataset(\n",
    "    sr=22050,\n",
    "    channels=1,\n",
    "    min_duration=12,\n",
    "    max_duration=640,\n",
    "    aug_shift=True,\n",
    "    sample_length=262144,\n",
    "    audio_files_dir=\"/home/jingyi49/multi-source-diffusion-models/data/slakh2100/test\",\n",
    "    stems=['bass', 'drums', 'guitar', 'piano'],)\n",
    "    \n",
    "dataloader = DataLoader(testset, batch_size=1, shuffle=True)\n",
    "    \n",
    "\n",
    "for data in dataloader:\n",
    "    x1 = data[:, 0, :].squeeze().reshape(1, 512, 512)\n",
    "    x2 = data[:, 1, :].squeeze().reshape(1, 512, 512)\n",
    "    x3 = data[:, 2, :].squeeze().reshape(1, 512, 512)\n",
    "    x4 = data[:, 3, :].squeeze().reshape(1, 512, 512)\n",
    "    #将4个乐器的图像加在一起\n",
    "    y = torch.sum(torch.stack([x1, x2, x3]), dim=0)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 262144])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#audio_tensor压缩为一个维度\n",
    "audio_tensor = audio_tensor.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "audio_tensor = y.reshape(1, 512*512)\n",
    "soundfile.write(file=\"/home/jingyi49/DiT/y.wav\", data=audio_tensor.squeeze(), samplerate=SAMPLE_RATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
